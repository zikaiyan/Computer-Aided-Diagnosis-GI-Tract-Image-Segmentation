{"cells":[{"cell_type":"markdown","id":"Pcd-9PDYytQu","metadata":{"id":"Pcd-9PDYytQu"},"source":["# Connect to Google Drive"]},{"cell_type":"code","execution_count":null,"id":"fuyvNAE_SYVl","metadata":{"id":"fuyvNAE_SYVl"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"id":"IHp48yTjSfRh","metadata":{"id":"IHp48yTjSfRh"},"outputs":[],"source":["import os\n","os.chdir('/content/drive/MyDrive/Portfolio/Projects/Course/Computer-Aided-Diagnosis-GI-Tract-Image-Segmentation')\n","!pwd"]},{"cell_type":"code","execution_count":null,"id":"S8aIgCGARYK6","metadata":{"id":"S8aIgCGARYK6"},"outputs":[],"source":["# Unzip files into the On the Fly Dataset\n","import zipfile\n","import os\n","\n","zip_path = '/content/drive/MyDrive/Portfolio/Projects/Course/Computer-Aided-Diagnosis-GI-Tract-Image-Segmentation/datasets/uw-madison-gi-tract-image-segmentation.zip'\n","extract_path = '/content/datasets'\n","\n","with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","    zip_ref.extractall(extract_path)\n","\n","print(\"Unzipped successfully!\")"]},{"cell_type":"markdown","id":"8557353f","metadata":{"id":"8557353f"},"source":["# Import Packages"]},{"cell_type":"code","execution_count":null,"id":"b80e01f6","metadata":{"id":"b80e01f6"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","from matplotlib import animation, rc\n","import imageio\n","import time\n","import cv2\n","from skimage.io import imread, imshow, imread_collection, concatenate_images\n","from skimage.transform import resize\n","import tensorflow as tf\n","from skimage.morphology import label\n","import random\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader, Dataset\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","from sklearn.model_selection import StratifiedKFold\n","from skimage.segmentation import watershed\n","from skimage.measure import label\n","import matplotlib.pyplot as plt\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import torch\n","import cv2\n","from albumentations.pytorch import ToTensorV2\n","import albumentations as A\n","import random\n","import pickle\n","import os\n","import collections\n","from sklearn.model_selection import train_test_split\n","import torchvision\n","from torchvision.transforms import ToPILImage\n","from torchvision.transforms import functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n","from tqdm import tqdm\n","import seaborn as sns\n","SEED = 42"]},{"cell_type":"code","execution_count":null,"id":"3bcbeb9b","metadata":{"id":"3bcbeb9b"},"outputs":[],"source":["# Fix randomness\n","\n","def fix_all_seeds(seed):\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","        torch.backends.cudnn.deterministic = True\n","\n","fix_all_seeds(SEED)"]},{"cell_type":"markdown","id":"piqzvFPOyj8S","metadata":{"id":"piqzvFPOyj8S"},"source":["# Config"]},{"cell_type":"code","execution_count":null,"id":"10E2zNbAyiqD","metadata":{"id":"10E2zNbAyiqD"},"outputs":[],"source":["PATH = '/content/datasets'\n","DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","BATCH_SIZE = 2\n","NUM_EPOCHS = 3\n","\n","WIDTH = 512\n","HEIGHT = 512\n","\n","resize_factor = True\n","flip_prob = 0.2\n","\n","# Normalize to resnet mean and std if True.\n","NORMALIZE = False\n","\n","RESNET_MEAN = (0.485, 0.456, 0.406)\n","RESNET_STD = (0.229, 0.224, 0.225)\n","\n","MOMENTUM = 0.9\n","LEARNING_RATE = 0.001\n","WEIGHT_DECAY = 0.0005\n","\n","# Changes the confidence required for a pixel to be kept for a mask.\n","cell_type_dict = {\"stomach\": 1, \"large_bowel\": 2, \"small_bowel\": 3}\n","idx_to_cell_type = {v: k for k, v in cell_type_dict.items()}\n","mask_threshold_dict = {1: 0.5, 2: 0.5, 3:  0.5}\n","min_score_dict = {1: 0.5, 2: 0.5, 3: 0.5}\n","\n","# Use a StepLR scheduler if True.\n","USE_SCHEDULER = False\n","\n","train_valid_ratio = 0.1\n","\n","BOX_DETECTIONS_PER_IMG = 100"]},{"cell_type":"markdown","id":"fb2867b2","metadata":{"id":"fb2867b2"},"source":["# Load Data"]},{"cell_type":"code","execution_count":null,"id":"UK-DPtQ2riHa","metadata":{"id":"UK-DPtQ2riHa"},"outputs":[],"source":["# load raw train data\n","raw_train_data = pd.read_csv('datasets/train.csv')\n","raw_train_data['case'] = raw_train_data['id'].apply(lambda x: x.split('_')[0])\n","raw_train_data['day'] = raw_train_data['id'].apply(lambda x: x.split('_')[1])\n","raw_train_data['slice'] = raw_train_data['id'].apply(lambda x: x.split('_')[2] + '_' + x.split('_')[3])\n","raw_train_data.head()"]},{"cell_type":"code","source":["# Create train/val/test split based on slice\n","temp = raw_train_data.groupby(['id','class']).agg({'segmentation':'count'}).reset_index().pivot_table(index = 'id', columns = 'class', values = 'segmentation').reset_index()\n","temp['all 3 organs'] = temp[['large_bowel','small_bowel','stomach']].sum(axis = 1)\n","\n","df_images_train, df_images_temp = train_test_split(temp, stratify=temp['all 3 organs'],\n","                                                  test_size=0.35,\n","                                                  random_state=SEED)\n","\n","df_images_val, df_images_test = train_test_split(df_images_temp, stratify=df_images_temp['all 3 organs'],\n","                                                  test_size=0.5714,\n","                                                  random_state=SEED) # test_size ~ 57.14% of 35%\n","\n","\n","df_train = raw_train_data[(raw_train_data['id'].isin(df_images_train['id'])) & (raw_train_data['segmentation'].notna())]\n","df_val = raw_train_data[(raw_train_data['id'].isin(df_images_val['id'])) & (raw_train_data['segmentation'].notna())]\n","df_test = raw_train_data[(raw_train_data['id'].isin(df_images_test['id'])) & (raw_train_data['segmentation'].notna())]"],"metadata":{"id":"-BvrOY9GyU-p"},"id":"-BvrOY9GyU-p","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"8ToVWXi2k200","metadata":{"id":"8ToVWXi2k200"},"outputs":[],"source":["# # Create train/val/test split based on mask\n","# # Extract information from data\n","# from src.data import SegmentationDataset\n","\n","# sd = SegmentationDataset(dataset_dir='/content/datasets/train',\n","#                          csv_file_path='/content/datasets/train.csv')\n","# df = sd.processed_df\n","\n","# random.seed(42)\n","\n","# # train - 65, val - 15, test - 20\n","# df_train, df_temp = train_test_split(df, test_size=0.35, random_state=42)\n","# df_val, df_test = train_test_split(df_temp, test_size=0.5714, random_state=42)  # test_size ~ 57.14% of 35%\n","\n","# df_train = df_train.reset_index(drop=True)\n","# df_val = df_val.reset_index(drop=True)\n","# df_test = df_test.reset_index(drop=True)"]},{"cell_type":"markdown","id":"46c0be7a","metadata":{"id":"46c0be7a"},"source":["# Data Preparation"]},{"cell_type":"code","execution_count":null,"id":"moaSn564yfxI","metadata":{"id":"moaSn564yfxI"},"outputs":[],"source":["# # Dice Loss\n","# class DiceLoss(nn.Module):\n","#     def __init__(self, smooth=1):\n","#         super(DiceLoss, self).__init__()\n","#         self.smooth = smooth\n","\n","#     def forward(self, y_pred, y_true):\n","#         y_pred = y_pred.contiguous().view(-1)\n","#         y_true = y_true.contiguous().view(-1)\n","#         intersection = (y_pred * y_true).sum()\n","#         dice = (2. * intersection + self.smooth) / (y_pred.sum() + y_true.sum() + self.smooth)\n","#         return 1 - dice\n","\n","# # Combined BCE and Dice Loss\n","# class BCEDiceLoss(nn.Module):\n","#     def __init__(self, smooth=1):\n","#         super(BCEDiceLoss, self).__init__()\n","#         self.bce = nn.BCEWithLogitsLoss()\n","#         self.dice = DiceLoss(smooth)\n","\n","#     def forward(self, y_pred, y_true):\n","#         bce_loss = self.bce(y_pred, y_true)\n","#         dice_loss = self.dice(y_pred, y_true)\n","#         return 0.5 * bce_loss + 0.5 * dice_loss\n","\n","# def dice_coef_func(y_true, y_pred, smooth=1):\n","#     y_true_f = y_true.contiguous().view(-1)\n","#     y_pred_f = y_pred.contiguous().view(-1)\n","#     intersection = torch.sum(y_true_f * y_pred_f)\n","#     return (2. * intersection + smooth) / (torch.sum(y_true_f) + torch.sum(y_pred_f) + smooth)\n","\n","# class AverageMeter(object):\n","#     \"\"\"Computes and stores the average and current value\"\"\"\n","#     def __init__(self):\n","#         self.reset()\n","\n","#     def reset(self):\n","#         self.val = 0\n","#         self.avg = 0\n","#         self.sum = 0\n","#         self.count = 0\n","\n","#     def update(self, val, n=1):\n","#         self.val = val\n","#         self.sum += val * n\n","#         self.count += n\n","#         self.avg = self.sum / self.count"]},{"cell_type":"code","execution_count":null,"id":"4a4afd71","metadata":{"id":"4a4afd71"},"outputs":[],"source":["# Helper functions\n","def rle_encoding(x):\n","    dots = np.where(x.flatten() == 1)[0]\n","    run_lengths = []\n","    prev = -2\n","    for b in dots:\n","        if (b>prev+1): run_lengths.extend((b + 1, 0))\n","        run_lengths[-1] += 1\n","        prev = b\n","    return ' '.join(map(str, run_lengths))\n","\n","\n","def remove_overlapping_pixels(mask, other_masks):\n","    for other_mask in other_masks:\n","        if np.sum(np.logical_and(mask, other_mask)) > 0:\n","            mask[np.logical_and(mask, other_mask)] = 0\n","    return mask\n","\n","def combine_masks(masks, mask_threshold):\n","    \"\"\"\n","    combine masks into one image\n","    \"\"\"\n","    masking = np.zeros((HEIGHT, WIDTH))\n","    # print(len(masks.shape), masks.shape)\n","    for m, mask in enumerate(masks,1):\n","        masking[mask>mask_threshold] = m\n","    return masking\n","\n","\n","def get_filtered_masks(pred):\n","    \"\"\"\n","    filter masks using MIN_SCORE for mask and MAX_THRESHOLD for pixels\n","    \"\"\"\n","    use_masks = []\n","    for i, mask in enumerate(pred[\"masks\"]):\n","\n","        # Filter-out low-scoring results. Not tried yet.\n","        scr = pred[\"scores\"][i].cpu().item()\n","        label = pred[\"labels\"][i].cpu().item()\n","        if scr > min_score_dict[label]:\n","            mask = mask.cpu().numpy().squeeze()\n","            # Keep only highly likely pixels\n","            binary_mask = mask > mask_threshold_dict[label]\n","            binary_mask = remove_overlapping_pixels(binary_mask, use_masks)\n","            use_masks.append(binary_mask)\n","\n","    return use_masks"]},{"cell_type":"code","execution_count":null,"id":"5ebbf0a0","metadata":{"id":"5ebbf0a0"},"outputs":[],"source":["#Metric: mean of the precision values at each IoU threshold\n","#Ref: https://www.kaggle.com/theoviel/competition-metric-map-iou\n","\n","def compute_iou(labels, y_pred, verbose=0):\n","    \"\"\"\n","    Computes the IoU for instance labels and predictions.\n","\n","    Args:\n","        labels (np array): Labels.\n","        y_pred (np array): predictions\n","\n","    Returns:\n","        np array: IoU matrix, of size true_objects x pred_objects.\n","    \"\"\"\n","\n","    true_objects = len(np.unique(labels))\n","    pred_objects = len(np.unique(y_pred))\n","\n","    if verbose:\n","        print(\"Number of true objects: {}\".format(true_objects))\n","        print(\"Number of predicted objects: {}\".format(pred_objects))\n","\n","    # Compute intersection between all objects\n","    intersection = np.histogram2d(\n","        labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects)\n","    )[0]\n","\n","    # Compute areas (needed for finding the union between all objects)\n","    area_true = np.histogram(labels, bins=true_objects)[0]\n","    area_pred = np.histogram(y_pred, bins=pred_objects)[0]\n","    area_true = np.expand_dims(area_true, -1)\n","    area_pred = np.expand_dims(area_pred, 0)\n","\n","    # Compute union\n","    union = area_true + area_pred - intersection\n","    intersection = intersection[1:, 1:] # exclude background\n","    union = union[1:, 1:]\n","    union[union == 0] = 1e-9\n","    iou = intersection / union\n","\n","    return iou\n","\n","def precision_at(threshold, iou):\n","    \"\"\"\n","    Computes the precision at a given threshold.\n","\n","    Args:\n","        threshold (float): Threshold.\n","        iou (np array): IoU matrix.\n","\n","    Returns:\n","        int: Number of true positives,\n","        int: Number of false positives,\n","        int: Number of false negatives.\n","    \"\"\"\n","    matches = iou > threshold\n","    true_positives = np.sum(matches, axis=1) == 1  # Correct objects\n","    false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n","    false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n","    tp, fp, fn = (\n","        np.sum(true_positives),\n","        np.sum(false_positives),\n","        np.sum(false_negatives),\n","    )\n","    return tp, fp, fn\n","\n","def iou_map(truths, preds, verbose=0):\n","    \"\"\"\n","    Computes IOU.\n","    Masks contain the segmented pixels where each object has one value associated,\n","    and 0 is the background.\n","\n","    Args:\n","        truths (list of masks): Ground truths.\n","        preds (list of masks): Predictions.\n","        verbose (int, optional): Whether to print infos. Defaults to 0.\n","\n","    Returns:\n","        float: mAP.\n","    \"\"\"\n","    ious = [compute_iou(truth, pred, verbose) for truth, pred in zip(truths, preds)]\n","\n","    if verbose:\n","        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n","\n","    prec = []\n","    for t in np.arange(0.5, 1.0, 0.05):\n","        tps, fps, fns = 0, 0, 0\n","        for iou in ious:\n","            tp, fp, fn = precision_at(t, iou)\n","            tps += tp\n","            fps += fp\n","            fns += fn\n","\n","        p = tps / (tps + fps + fns)\n","        prec.append(p)\n","\n","        if verbose:\n","            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tps, fps, fns, p))\n","\n","    if verbose:\n","        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n","\n","    return np.mean(prec)\n","\n","\n","def get_score(ds, mdl, threshold_dict):\n","    \"\"\"\n","    Get average IOU mAP score for a dataset\n","    \"\"\"\n","    mdl.eval()\n","    iouscore = 0\n","    for i in tqdm(range(len(ds))):\n","        img, targets = ds[i]\n","        with torch.no_grad():\n","            result = mdl([img.to(DEVICE)])[0]\n","\n","        masks = combine_masks(targets['masks'], 0.5)\n","        labels = pd.Series(result['labels'].cpu().numpy()).value_counts()\n","\n","        mask_threshold = threshold_dict[labels.sort_values().index[-1]]\n","        pred_masks = combine_masks(get_filtered_masks(result), mask_threshold)\n","        iouscore += iou_map([masks],[pred_masks])\n","    return iouscore / len(ds)"]},{"cell_type":"code","execution_count":null,"id":"049ba205","metadata":{"id":"049ba205"},"outputs":[],"source":["class Compose:\n","    def __init__(self, transforms):\n","        self.transforms = transforms\n","\n","    def __call__(self, image, target):\n","        for t in self.transforms:\n","            image, target = t(image, target)\n","        return image, target\n","\n","class VerticalFlip:\n","    def __init__(self, prob):\n","        self.prob = prob\n","\n","    def __call__(self, image, target):\n","        if random.random() < self.prob:\n","            height, width = image.shape[-2:]\n","            image = image.flip(-2)\n","            bbox = target[\"boxes\"]\n","            bbox[:, [1, 3]] = height - bbox[:, [3, 1]]\n","            target[\"boxes\"] = bbox\n","            target[\"masks\"] = target[\"masks\"].flip(-2)\n","        return image, target\n","\n","class HorizontalFlip:\n","    def __init__(self, prob):\n","        self.prob = prob\n","\n","    def __call__(self, image, target):\n","        if random.random() < self.prob:\n","            height, width = image.shape[-2:]\n","            image = image.flip(-1)\n","            bbox = target[\"boxes\"]\n","            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n","            target[\"boxes\"] = bbox\n","            target[\"masks\"] = target[\"masks\"].flip(-1)\n","        return image, target\n","\n","class Normalize:\n","    def __call__(self, image, target):\n","        image = F.normalize(image, RESNET_MEAN, RESNET_STD)\n","        return image, target\n","\n","class ToTensor:\n","    def __call__(self, image, target):\n","        image = F.to_tensor(image)\n","        return image, target\n","\n","\n","def get_transform(train):\n","    transforms = [ToTensor()]\n","    if NORMALIZE:\n","        transforms.append(Normalize())\n","\n","    # Data augmentation for train\n","    if train:\n","        transforms.append(HorizontalFlip(flip_prob))\n","        transforms.append(VerticalFlip(flip_prob))\n","\n","    return Compose(transforms)"]},{"cell_type":"code","execution_count":null,"id":"deffbf3a","metadata":{"id":"deffbf3a"},"outputs":[],"source":["class ImageDataset(Dataset):\n","    def __init__(self, image_dir, df, train_flag=None, transforms=None, resize=False):\n","        self.transforms = transforms\n","        self.image_dir = image_dir\n","        self.df = df\n","        self.train_flag = train_flag\n","\n","        self.should_resize = resize is not False\n","        if self.should_resize:\n","            self.height = int(HEIGHT * resize)\n","            self.width = int(WIDTH * resize)\n","            print(\"image size used:\", self.height, self.width)\n","        else:\n","            self.height = HEIGHT\n","            self.width = WIDTH\n","\n","        self.image_info = collections.defaultdict(dict)\n","\n","        df_temp = self.df.groupby([\"id\"])['segmentation'].agg(lambda x: list(x)).reset_index().merge(self.df.groupby([\"id\"])['class'].agg(lambda x: list(x)).reset_index(), how = 'left', on = 'id')\n","\n","        for index, row in df_temp.iterrows():\n","            sample_id = row['id']\n","            case_num = sample_id.split('_')[0]\n","            day_num = sample_id.split('_')[1]\n","            slice_num = [x for x in os.listdir(f'/content/datasets/train/{case_num}/{case_num + \"_\" + day_num}/scans/') if sample_id.split('_')[3] in x.split('_')[1]][0]\n","            path = f'{self.image_dir}/train/{case_num}/{case_num + \"_\" + day_num}/scans/{slice_num}'\n","            self.image_info[index] = {\n","                    'image_id': row['id'],\n","                    'image_path': path,\n","                    'annotations': row[\"segmentation\"],\n","                    'class': [cell_type_dict[x] for x in row[\"class\"]]\n","                    }\n","\n","    def get_box(self, a_mask):\n","        ''' Get the bounding box of a given mask '''\n","        pos = np.where(a_mask)\n","        xmin = np.min(pos[1])\n","        xmax = np.max(pos[1])\n","        ymin = np.min(pos[0])\n","        ymax = np.max(pos[0])\n","        return [xmin, ymin, xmax, ymax]\n","\n","    def __getitem__(self, idx):\n","        ''' Get the image and the target'''\n","\n","        img_path = self.image_info[idx][\"image_path\"]\n","        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n","        initial_HEIGHT = img.shape[0]\n","        initial_WIDTH = img.shape[1]\n","\n","        if self.should_resize:\n","            img = cv2.resize(img, (self.width, self.height))\n","\n","        info = self.image_info[idx]\n","\n","        n_objects = len(info['annotations'])\n","        masks = np.zeros((len(info['annotations']), self.height, self.width), dtype=np.uint8)\n","        boxes = []\n","        labels = []\n","        for i, annotation in enumerate(info['annotations']):\n","            a_mask = rle_decode(annotation, (initial_HEIGHT, initial_WIDTH))\n","\n","            if self.should_resize:\n","                a_mask = cv2.resize(a_mask, (self.width, self.height))\n","\n","            a_mask = np.array(a_mask) > 0\n","            masks[i, :, :] = a_mask\n","\n","            boxes.append(self.get_box(a_mask))\n","\n","        # labels\n","        labels = [int(info[\"class\"][m]) for m in range(0, (n_objects))]\n","\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        labels = torch.as_tensor(labels, dtype=torch.int64)\n","        masks = torch.as_tensor(masks, dtype=torch.uint8)\n","\n","        image_id = torch.tensor([idx])\n","        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","        iscrowd = torch.zeros((n_objects,), dtype=torch.int64)\n","\n","        if self.train_flag == 'train':\n","            # This is the required target for the Mask R-CNN\n","            target = {\n","                'boxes': boxes,\n","                'labels': labels,\n","                'masks': masks,\n","                'image_id': image_id,\n","                'area': area,\n","                'iscrowd': iscrowd\n","            }\n","        else:\n","            target = {\n","                'boxes': boxes,\n","                'labels': labels,\n","                'masks': masks,\n","                'image_id': image_id,\n","                'area': area,\n","                'iscrowd': iscrowd,\n","                'image_path':img_path,\n","                'initial_HEIGHT': initial_HEIGHT,\n","                'initial_WIDTH': initial_WIDTH\n","            }\n","\n","        if self.transforms is not None:\n","            img, target = self.transforms(img, target)\n","\n","        return img, target\n","\n","    def __len__(self):\n","        return len(self.image_info)"]},{"cell_type":"code","execution_count":null,"id":"9tF2ufK4nCEK","metadata":{"id":"9tF2ufK4nCEK"},"outputs":[],"source":["ds_train = ImageDataset(PATH, df_train, train_flag = 'train', resize=resize_factor, transforms=get_transform(train=False))\n","dl_train = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True,\n","                      num_workers=2, collate_fn=lambda x: tuple(zip(*x)))\n","\n","ds_val = ImageDataset(PATH, df_val, train_flag = 'train', resize=resize_factor, transforms=get_transform(train=False))\n","dl_val = DataLoader(ds_val, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True,\n","                    num_workers=2, collate_fn=lambda x: tuple(zip(*x)))\n","\n","ds_test = ImageDataset(PATH, df_test, train_flag = 'train', resize=resize_factor, transforms=get_transform(train=False))\n","dl_test = DataLoader(ds_test, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True,\n","                     num_workers=2, collate_fn=lambda x: tuple(zip(*x)))"]},{"cell_type":"code","source":["ds_train[0][1]"],"metadata":{"id":"nJnkaFHW4L-q"},"id":"nJnkaFHW4L-q","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"6859f408","metadata":{"id":"6859f408"},"source":["# Visualization"]},{"cell_type":"code","execution_count":null,"id":"ff7ceb2d","metadata":{"id":"ff7ceb2d"},"outputs":[],"source":["# Helper functions\n","def rle_decode(mask_rle, shape, color=1):\n","    '''\n","    mask_rle: run-length as string formated (start length)\n","    shape: (height, width, channels) of array to return\n","    color: color for the mask\n","    Returns numpy array (mask)\n","    '''\n","    s = mask_rle.split()\n","\n","    starts = list(map(lambda x: int(x) - 1, s[0::2]))\n","    lengths = list(map(int, s[1::2]))\n","    ends = [x + y for x, y in zip(starts, lengths)]\n","\n","    if len(shape)==3:\n","        img = np.zeros((shape[0] * shape[1], shape[2]), dtype=np.float32)\n","    else:\n","        img = np.zeros(shape[0] * shape[1], dtype=np.float32)\n","\n","    for start, end in zip(starts, ends):\n","        img[start : end] = color\n","\n","    return img.reshape(shape)"]},{"cell_type":"code","execution_count":null,"id":"inp0SlQax6Rh","metadata":{"id":"inp0SlQax6Rh"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from matplotlib.colors import ListedColormap\n","import matplotlib.patches as mpatches\n","\n","def plot_image(image, target=None):\n","    # Create custom colormaps for the masks\n","    cmap1 = ListedColormap(['none', 'red'])  # Mask 1 in red\n","    cmap2 = ListedColormap(['none', 'green'])  # Mask 2 in green\n","    cmap3 = ListedColormap(['none', 'blue'])  # Mask 3 in blue\n","\n","    fig, ax = plt.subplots()\n","    # Display the grayscale image\n","    ax.imshow((image)[0, :, :], cmap='gray')\n","\n","    if target is not None:\n","        # Display masks\n","        for i in range(len(target['labels'])):\n","            if target['labels'][i] == 3: # Large Bowel\n","                ax.imshow(target['masks'][i, :, :], cmap=cmap1, alpha=0.5)\n","            elif target['labels'][i] == 2: # Small Bowl\n","                ax.imshow(target['masks'][i, :, :], cmap=cmap2, alpha=0.5)\n","            else: # Stomach\n","                ax.imshow(target['masks'][i, :, :], cmap=cmap3, alpha=0.5)\n","\n","        # Create a legend for the masks\n","        red_patch = mpatches.Patch(color='red', label='Small Bowel')\n","        green_patch = mpatches.Patch(color='green', label='Large Bowel')\n","        blue_patch = mpatches.Patch(color='blue', label='Stomach')\n","        plt.legend(handles=[red_patch, green_patch, blue_patch])\n","\n","    # Show the plot\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"id":"-QLCYzFYx7GF","metadata":{"id":"-QLCYzFYx7GF"},"outputs":[],"source":["idx =100 # Change index to see different slice\n","image = ds_train[idx][0]\n","target = ds_train[idx][1]\n","plot_image(image, target)"]},{"cell_type":"code","execution_count":null,"id":"b135a1f9","metadata":{"id":"b135a1f9"},"outputs":[],"source":["# # Define a function to plot images and their corresponding masks\n","# def plot_images_and_masks(sample_id):\n","#     # Extract the case number, day number, and slice information from the sample ID\n","#     case_num = sample_id.split('_')[0]\n","#     day_num = sample_id.split('_')[1]\n","#     slice_num = [x for x in os.listdir(f'/content/datasets/train/{case_num}/{case_num + \"_\" + day_num}/scans/') if sample_id.split('_')[3] in x.split('_')[1]][0]\n","#     path = f'/content/datasets/train/{case_num}/{case_num + \"_\" + day_num}/scans/{slice_num}'\n","\n","#     # Load the image data from the specified path\n","#     image_data = plt.imread(path)\n","\n","#     # Display the sample ID\n","#     print('\\nID:', sample_id,'\\n')\n","\n","#     # If using pandas to handle data, this code would be relevant to display data records associated with the sample ID\n","#     print('\\nTrain data records:')\n","#     display(raw_train_data[(raw_train_data['id']==sample_id) & (raw_train_data['segmentation'].notna())].reset_index(drop = True))\n","\n","#     # Display image data characteristics\n","#     print('\\nImage data shape:', image_data.shape)\n","#     # print('Min and max pixels:', image_data.min(),',', image_data.max(),'\\n')\n","\n","#     # Plot the original image\n","#     plt.subplot(1, 2, 1)\n","#     plt.imshow(image_data, cmap = 'gray')\n","#     plt.title('Input image')\n","#     plt.axis(\"off\")\n","\n","#     # Plot the image with masks overlaid\n","#     plt.subplot(1, 2, 2)\n","#     plt.imshow(image_data, cmap = 'gray')\n","#     for i in range(0, raw_train_data[(raw_train_data['id']==sample_id) & (raw_train_data['segmentation'].notna())].shape[0]):\n","#         plt.imshow(rle_decode(raw_train_data[(raw_train_data['id']==sample_id) & (raw_train_data['segmentation'].notna())]['segmentation'].tolist()[i], shape = image_data.shape), alpha = 0.4, cmap ='gray')\n","#     plt.title('Input image with mask')\n","#     plt.axis(\"off\")\n","#     plt.tight_layout()\n","#     plt.show()\n","\n","# # Example usage\n","# sample_id = 'case123_day0_slice_0111'\n","# plot_images_and_masks(sample_id)\n"]},{"cell_type":"code","execution_count":null,"id":"5e2a2bad","metadata":{"id":"5e2a2bad"},"outputs":[],"source":["# rc('animation', html='jshtml')\n","\n","# def create_animation(ims):\n","#     print('# Images:', len(ims))\n","#     fig = plt.figure(figsize=(6, 6))\n","#     plt.axis('off')\n","#     im = plt.imshow(ims[0], cmap=\"gray\")\n","\n","#     def animate_func(i):\n","#         im.set_array(ims[i])\n","#         return [im]\n","\n","#     return animation.FuncAnimation(fig, animate_func, frames = len(ims), interval = 1)"]},{"cell_type":"code","execution_count":null,"id":"bbf9d494","metadata":{"id":"bbf9d494"},"outputs":[],"source":["# # See images for a case number and day number\n","# case_num = 'case123'\n","# day_num = 'day0'\n","# list_images_data = []\n","# for images in os.listdir(f'/content/datasets/train/{case_num}/{case_num}_{day_num}/scans/'):\n","#     image_data = plt.imread(f'/content/datasets/train/{case_num}/{case_num}_{day_num}/scans/' + images)\n","#     list_images_data.append(image_data)\n","# create_animation(list_images_data)"]},{"cell_type":"markdown","id":"a9cc3d8c","metadata":{"id":"a9cc3d8c"},"source":["# Modelling"]},{"cell_type":"markdown","id":"ca63f0bd","metadata":{"id":"ca63f0bd"},"source":["We need to build a model to identify where healthy organs (stomach, large and small bowels) are present in an MRI scan. We would be building a mask R-CNN model to do this."]},{"cell_type":"markdown","id":"LbaiuQ1CzTYN","metadata":{"id":"LbaiuQ1CzTYN"},"source":["## Model Loading"]},{"cell_type":"code","execution_count":null,"id":"c237edb8","metadata":{"id":"c237edb8"},"outputs":[],"source":["def get_model(num_classes, model_chkpt=None):\n","    # This is just a dummy value for the classification head\n","\n","    if NORMALIZE:\n","        model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True,\n","                                                                   box_detections_per_img=BOX_DETECTIONS_PER_IMG,\n","                                                                   image_mean=RESNET_MEAN,\n","                                                                   image_std=RESNET_STD)\n","    else:\n","        model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True,\n","                                                                   box_detections_per_img=BOX_DETECTIONS_PER_IMG)\n","\n","    # get the number of input features for the classifier\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    # replace the pre-trained head with a new one\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes+1)\n","\n","    # now get the number of input features for the mask classifier\n","    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n","    hidden_layer = 256\n","    # and replace the mask predictor with a new one\n","    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes+1)\n","\n","    if model_chkpt:\n","        model.load_state_dict(torch.load(model_chkpt, map_location=DEVICE))\n","    return model"]},{"cell_type":"code","execution_count":null,"id":"vMH7wiJhE1_A","metadata":{"id":"vMH7wiJhE1_A"},"outputs":[],"source":["# Get the Mask R-CNN model\n","# The model does classification, bounding boxes and MASKs for individuals, all at the same time\n","# We only care about MASKS\n","\n","# Initialize a new model or load a trained model for further training\n","e = 0\n","\n","if e > 0:\n","    model_chk = f\"mask-rcnn-trained/pytorch_model-e{e}.bin\"\n","    print(\"Loading:\", model_chk)\n","    model = get_model(len(cell_type_dict), model_chk)\n","    model.load_state_dict(torch.load(model_chk))\n","    model = model.to(DEVICE)\n","else:\n","    model = get_model(len(cell_type_dict))\n","    model = model.to(DEVICE)\n","\n","# Turn model into training mode\n","for param in model.parameters():\n","    param.requires_grad = True\n","model.train()"]},{"cell_type":"markdown","id":"624dcf77","metadata":{"id":"624dcf77"},"source":["## Model Training"]},{"cell_type":"code","execution_count":null,"id":"9fdb9e02","metadata":{"id":"9fdb9e02"},"outputs":[],"source":["TRAIN = True"]},{"cell_type":"code","execution_count":null,"id":"12b08a0c","metadata":{"id":"12b08a0c"},"outputs":[],"source":["if TRAIN:\n","    # Initialize\n","    params = [p for p in model.parameters() if p.requires_grad]\n","    optimizer = torch.optim.SGD(params, lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n","    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n","\n","    # Number of batches\n","    n_batches, n_batches_val = len(dl_train), len(dl_val)\n","\n","    # Tract Loss\n","    epoch_nbr = []\n","    train_losses = []\n","    val_losses = []\n","\n","    for epoch in range(1, NUM_EPOCHS + 1):\n","        # Set to train mode\n","        model.train()\n","\n","        print(f\"Starting epoch {epoch} of {NUM_EPOCHS}\")\n","        epoch_nbr.append(epoch)\n","\n","        time_start = time.time()\n","        loss_accum = 0.0\n","        loss_mask_accum = 0.0\n","        loss_classifier_accum = 0.0\n","        for batch_idx, (images, targets) in enumerate(tqdm(dl_train), 1):\n","            # Predict\n","            images = list(image.to(DEVICE) for image in images)\n","            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n","\n","            # Forward pass\n","            loss_dict = model(images, targets)\n","            loss = loss_dict['loss_mask']\n","\n","            # Backprop\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Logging\n","            loss_accum += loss.item()\n","\n","            if batch_idx % 500 == 0:\n","                print(f\"[Batch {batch_idx:3d} / {n_batches:3d}] Batch train loss: {loss.item():7.3f}. {time.time() - time_start:.0f} secs\")\n","\n","        if USE_SCHEDULER:\n","            lr_scheduler.step()\n","\n","        # Train losses\n","        train_loss = loss_accum / n_batches\n","\n","        # Store train losses\n","        train_losses.append(train_loss)\n","\n","        # Validation\n","        val_loss_accum = 0\n","\n","        with torch.no_grad():\n","            for batch_idx, (images, targets) in enumerate(tqdm(dl_val), 1):  # Use tqdm here\n","                images = list(image.to(DEVICE) for image in images)\n","                targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n","\n","                val_loss_dict = model(images, targets)\n","                val_batch_loss = val_loss_dict['loss_mask']\n","                val_loss_accum += val_batch_loss.item()\n","\n","        # Validation losses\n","        val_loss = val_loss_accum / n_batches_val\n","        elapsed = time.time() - time_start\n","\n","        # Store validation losses\n","        val_losses.append(val_loss)\n","\n","        # Save model\n","        torch.save(model.state_dict(), f\"mask-rcnn-trained/pytorch_model-e{e + epoch}.bin\")\n","\n","        # Print result\n","        prefix = f\"[Epoch {epoch:2d} / {NUM_EPOCHS:2d}]\"\n","        print(prefix)\n","        print(f\"{prefix} Train loss: {train_loss:7.3f}. Val loss: {val_loss:7.3f} [{elapsed:.0f} secs]\")\n","\n","    # Export Losses\n","    losses = {\n","        'epoch_nbr': epoch_nbr,\n","        'train_losses': train_losses,\n","        'val_losses': val_losses,\n","    }\n","    df_losses = pd.DataFrame(losses)\n","    df_losses.to_csv(\"output/mask_rcnn_losses.csv\", index=False)"]},{"cell_type":"markdown","id":"10UaPpAk_uSF","metadata":{"id":"10UaPpAk_uSF"},"source":["## Visualize Training Process"]},{"cell_type":"code","execution_count":null,"id":"8-gBQwlc_yT-","metadata":{"id":"8-gBQwlc_yT-"},"outputs":[],"source":["# Load the data\n","df_losses = pd.read_csv(\"output/mask_rcnn_losses.csv\")\n","\n","# Plotting\n","plt.figure(figsize=(15, 10))\n","\n","# Losses Plot\n","plt.subplot(3, 1, 1)\n","sns.lineplot(data=df_losses, x='epoch_nbr', y='train_losses', label='Train Losses')\n","sns.lineplot(data=df_losses, x='epoch_nbr', y='val_losses', label='Validation Losses')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Total Losses')\n","\n","# Mask Losses Plot\n","plt.subplot(3, 1, 2)\n","sns.lineplot(data=df_losses, x='epoch_nbr', y='train_mask_losses', label='Train Mask Losses')\n","sns.lineplot(data=df_losses, x='epoch_nbr', y='val_mask_losses', label='Validation Mask Losses')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Mask Losses')\n","\n","# Classifier Losses Plot\n","plt.subplot(3, 1, 3)\n","sns.lineplot(data=df_losses, x='epoch_nbr', y='train_classifier_losses', label='Train Classifier Losses')\n","sns.lineplot(data=df_losses, x='epoch_nbr', y='val_classifier_losses', label='Validation Classifier Losses')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Classifier Losses')\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","id":"e39b4d0b","metadata":{"id":"e39b4d0b"},"source":["## Result Validation"]},{"cell_type":"code","execution_count":null,"id":"87a4c8b1","metadata":{"id":"87a4c8b1"},"outputs":[],"source":["# Load trained model\n","e = 10\n","model_chk = f\"mask-rcnn-trained/pytorch_model-e{e}.bin\"\n","print(\"Loading:\", model_chk)\n","model = get_model(len(cell_type_dict), model_chk)\n","model.load_state_dict(torch.load(model_chk))\n","model = model.to(DEVICE)"]},{"cell_type":"code","execution_count":null,"id":"hxWQOjp7FHJQ","metadata":{"id":"hxWQOjp7FHJQ"},"outputs":[],"source":["def compare_truth_pred(model, dataset, sample_index):\n","    # Create custom colormaps for the masks\n","    cmap1 = ListedColormap(['none', 'red'])  # Mask 1 in red\n","    cmap2 = ListedColormap(['none', 'green'])  # Mask 2 in green\n","    cmap3 = ListedColormap(['none', 'blue'])  # Mask 3 in blue\n","    red_patch = mpatches.Patch(color='red', label='Small Bowel')\n","    green_patch = mpatches.Patch(color='green', label='Large Bowel')\n","    blue_patch = mpatches.Patch(color='blue', label='Stomach')\n","\n","    # Retrieve the image and target (labels and masks) for the given index from the training dataset\n","    img, target = dataset[sample_index]\n","    image_id = target['image_id']  # ID of the image\n","    initial_HEIGHT = target['initial_HEIGHT']  # Original height of the image\n","    initial_WIDTH = target['initial_WIDTH']  # Original width of the image\n","\n","    # Set up a matplotlib figure with three subplots to display the original image, ground truth, and predictions\n","    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20,60), facecolor=\"#fefefe\")\n","\n","    # Plot original image\n","    ax[0].imshow(img[0,:,:], cmap = 'gray')\n","    ax[0].set_title(f\"Original Image (Id:{sample_index})\")\n","    ax[0].axis(\"off\")\n","\n","    # Plot ground truth masks\n","    ax[1].imshow(img[0,:,:], cmap = 'gray')\n","    ax[1].set_title(f\"Ground Truth (Id:{sample_index})\")\n","    ax[1].axis(\"off\")\n","\n","    # Display masks\n","    for i in range(len(target['labels'])):\n","        if target['labels'][i] == 3: # Large Bowel\n","            ax[1].imshow(target['masks'][i, :, :], cmap=cmap1, alpha=0.5)\n","        elif target['labels'][i] == 2: # Small Bowl\n","            ax[1].imshow(target['masks'][i, :, :], cmap=cmap2, alpha=0.5)\n","        else: # Stomach\n","            ax[1].imshow(target['masks'][i, :, :], cmap=cmap3, alpha=0.5)\n","\n","    # Create a legend for the masks\n","    ax[1].legend(handles=[red_patch, green_patch, blue_patch])\n","\n","    # Plot predicted masks\n","    ax[2].imshow(img[0,:,:], cmap = 'gray')\n","    ax[2].set_title(f\"Predictions (Id:{sample_index})\")\n","    ax[2].axis(\"off\")\n","\n","    # Set the model to evaluation mode to disable layers like dropout and batch normalization\n","    model.eval()\n","    with torch.no_grad():\n","        # Make predictions using the model\n","        preds = model([img.to(DEVICE)])[0]\n","\n","    # Process results for each type of cell\n","    for cell_type in range(1, len(min_score_dict)+1):\n","        previous_masks = []  # List to store previous masks for overlap removal\n","        for i, mask in enumerate(preds[\"masks\"]):\n","            # Filter out results with a score below the threshold\n","            score = preds[\"scores\"][i].cpu().item()\n","            label = preds[\"labels\"][i].cpu().item()\n","            if (score > min_score_dict[label]) and (label == cell_type):\n","                mask = mask.cpu().numpy()  # Convert mask to numpy array\n","                # Threshold mask to create binary mask\n","                binary_mask = mask > mask_threshold_dict[label]\n","                # Remove overlapping pixels\n","                binary_mask = remove_overlapping_pixels(binary_mask, previous_masks)\n","                previous_masks.append(binary_mask)\n","\n","        # Flatten list of previous masks and create a binary mask for all cells of a type\n","        previous_masks = torch.tensor([item for sublist in previous_masks for item in sublist])\n","        binary_mask = np.zeros((HEIGHT, WIDTH))\n","        for m, mask in enumerate(previous_masks, 1):\n","            binary_mask[mask > 0.5] = 1\n","\n","        # Display masks\n","        if cell_type == 3: # Large Bowel\n","            ax[2].imshow(binary_mask, cmap=cmap1, alpha=0.5)\n","        elif cell_type == 2: # Small Bowl\n","            ax[2].imshow(binary_mask, cmap=cmap2, alpha=0.5)\n","        else: # Stomach\n","            ax[2].imshow(binary_mask, cmap=cmap3, alpha=0.5)\n","\n","    # Create a legend for the masks\n","    ax[2].legend(handles=[red_patch, green_patch, blue_patch])\n","\n","    # Show the plot\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"id":"BtOc94R-YBPX","metadata":{"id":"BtOc94R-YBPX"},"outputs":[],"source":["# Example usage:\n","for index in range(500, 510):\n","  compare_truth_pred(model, ds_val, index)"]},{"cell_type":"markdown","id":"X2F_wEgC6YYv","metadata":{"id":"X2F_wEgC6YYv"},"source":["## Find Best Threshold"]},{"cell_type":"code","execution_count":null,"id":"llgCj1oT6cmE","metadata":{"id":"llgCj1oT6cmE"},"outputs":[],"source":["FIND_THRESHOLD = False"]},{"cell_type":"code","execution_count":null,"id":"22e0c240","metadata":{"id":"22e0c240"},"outputs":[],"source":["# Find best threshold based on validation dataset for each organ\n","if FIND_THRESHOLD:\n","  valid_data_threshold_analysis = pd.DataFrame(data = None, columns = ['stomach', 'large_bowel', 'small_bowel', 'IoU'])\n","  i = 0\n","  for stomach_prob in tqdm(range(45, 55, 5), desc='Stomach Probabilities'):\n","      for large_bowel_prob in range(45, 55, 5):\n","          for small_bowel_prob in range(45, 55, 5):\n","              mask_threshold_dict = {1: stomach_prob / 100, 2: large_bowel_prob / 100, 3: small_bowel_prob / 100}\n","              valid_data_threshold_analysis.loc[i] = stomach_prob / 100, large_bowel_prob / 100, small_bowel_prob / 100, get_score(ds_val, model, mask_threshold_dict)\n","              i += 1\n","  valid_data_threshold_analysis"]},{"cell_type":"markdown","id":"160b3cf3","metadata":{"id":"160b3cf3"},"source":["# Inference"]},{"cell_type":"code","execution_count":null,"id":"d11a1b1d","metadata":{"id":"d11a1b1d"},"outputs":[],"source":["def inference(model, dataset):\n","    # Froze model parameters and set it to evaluation mode\n","    for param in model.parameters():\n","        param.requires_grad = False\n","\n","    model.eval();\n","\n","    # Initialize a list to store prediction results\n","    predictions = []\n","    true_masks = []\n","    pred_masks = []\n","\n","    # Iterate over the dataset using tqdm for progress bar visualization\n","    for num in tqdm(range(0, len(dataset))):\n","        # Retrieve the image for the current sample\n","        sample = dataset[num]\n","        img = sample[0]  # The image data\n","        image_id = sample[1]['image_id']  # ID of the image\n","        initial_HEIGHT = sample[1]['initial_HEIGHT']  # Original height of the image\n","        initial_WIDTH = sample[1]['initial_WIDTH']  # Original width of the image\n","\n","        # Append masks\n","        true_masks.append(np.zeros((3, HEIGHT, WIDTH)))\n","        pred_masks.append(np.zeros((3, HEIGHT, WIDTH)))\n","        # Add ground truth masks to the list\n","        for i in range(len(sample[1]['labels'])):\n","            true_masks[num][sample[1]['labels'][i]-1, :, :] = sample[1]['masks'][i, :, :]\n","\n","\n","        # No gradient is needed for inference, thus wrap in torch.no_grad()\n","        with torch.no_grad():\n","            preds = model([img.to(DEVICE)])[0]  # Perform inference\n","\n","        # Process predictions for each type of cell\n","        for cell_type in range(1, len(min_score_dict)+1):\n","            previous_masks = []  # List to store previous masks for overlap removal\n","            for i, mask in enumerate(preds[\"masks\"]):\n","                # Filter out predictions with a score below the threshold\n","                score = preds[\"scores\"][i].cpu().item()\n","                label = preds[\"labels\"][i].cpu().item()\n","                if (score > min_score_dict[label]) and (label == cell_type):\n","                    mask = mask.cpu().numpy()  # Convert mask to numpy array\n","                    # Resize mask back to the original image dimensions\n","                    mask = cv2.resize(mask.reshape((HEIGHT, WIDTH, 1)), (initial_WIDTH, initial_HEIGHT))[None, :, :]\n","                    # Threshold mask to create binary mask\n","                    binary_mask = mask > mask_threshold_dict[label]\n","                    # Remove overlapping pixels\n","                    binary_mask = remove_overlapping_pixels(binary_mask, previous_masks)\n","                    previous_masks.append(binary_mask)\n","\n","            # Flatten list of previous masks and create a binary mask for all cells of a type\n","            previous_masks = torch.tensor([np.array(item, dtype=int) for sublist in previous_masks for item in sublist])\n","            binary_mask = np.zeros((initial_HEIGHT, initial_WIDTH))\n","            for m, mask in enumerate(previous_masks, 1):\n","                binary_mask[mask > 0.5] = 1\n","            # Encode the binary mask using run-length encoding\n","            rle = rle_encoding(binary_mask)\n","            # Append the prediction (image ID, cell type, and RLE)\n","            predictions.append((image_id, {v: k for k, v in cell_type_dict.items()}[cell_type], rle))\n","            # Add the predicted mask to the list\n","            binary_mask_resized = cv2.resize(binary_mask, (HEIGHT, WIDTH), interpolation=cv2.INTER_NEAREST)\n","            pred_masks[num][cell_type-1, :, :] = binary_mask_resized[:, :]\n","\n","        # Add empty prediction if no RLE was generated for this image\n","        all_image_ids = [image_id for image_id, _, _ in predictions]\n","        if image_id not in all_image_ids:\n","            for key in cell_type_dict.keys():\n","                predictions.append((image_id, key, \"\"))\n","\n","    # Convert predictions to DataFrame\n","    df_preds = pd.DataFrame(predictions, columns=['id', 'class', 'predicted'])\n","\n","    return df_preds, true_masks, pred_masks"]},{"cell_type":"code","execution_count":null,"id":"5082e8c7","metadata":{"id":"5082e8c7"},"outputs":[],"source":["# Load data for inference\n","# ds_train_inf = ImageDataset(PATH, df_train, train_flag = None, resize=resize_factor, transforms=get_transform(train=False))\n","ds_val_inf = ImageDataset(PATH, df_val, train_flag = None, resize=resize_factor, transforms=get_transform(train=False))\n","# ds_test_inf = ImageDataset(PATH, df_test, train_flag = None, resize=resize_factor, transforms=get_transform(train=False))\n","\n","# Make predictions\n","# df_preds_train, true_masks_train, pred_masks_train = inference(model, ds_train_inf)\n","df_preds_val, true_masks_val, pred_masks_val = inference(model, ds_val_inf)\n","# df_preds_test, true_masks_test, pred_masks_test = inference(model, ds_test_inf)\n","\n","# df_preds_train.to_csv(\"output/maskrcnn_preds_train.csv\", index=False)\n","df_preds_val.to_csv(f\"output/maskrcnn_preds_val.csv\", index=False)\n","# df_preds_test.to_csv(\"output/maskrcnn_preds_test.csv\", index=False)\n","\n","display(df_preds_val.head())"]},{"cell_type":"code","execution_count":null,"id":"6b7RY4BsMQ_i","metadata":{"id":"6b7RY4BsMQ_i"},"outputs":[],"source":["# Calculate IoU for three categories\n","iou_stomach = iou_map([mask[0, :, :] for mask in true_masks_val], [mask[0, :, :] for mask in pred_masks_val])\n","iou_large_bowel = iou_map([mask[1, :, :] for mask in true_masks_val], [mask[1, :, :] for mask in pred_masks_val])\n","iou_small_bowel = iou_map([mask[2, :, :] for mask in true_masks_val], [mask[2, :, :] for mask in pred_masks_val])\n","print(f\"Stomach IoU: {iou_stomach}\")\n","print(f\"Large Bowel IoU: {iou_large_bowel}\")\n","print(f\"Small Bowel IoU: {iou_small_bowel}\")"]},{"cell_type":"code","source":[],"metadata":{"id":"gUh137zaaLra"},"id":"gUh137zaaLra","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":23725.326068,"end_time":"2022-05-18T22:18:04.420867","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-05-18T15:42:39.094799","version":"2.3.4"}},"nbformat":4,"nbformat_minor":5}